{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "# from llm.openai_endpoints import query_openai_model\n",
    "from llm.endpoints.openai_endpoint import OpenAIEndpoint, OpenAIModelType\n",
    "from llm.annotator_helpers import get_model_annotations\n",
    "from utils.parsing_helpers import parse_yago_uri\n",
    "\n",
    "\n",
    "from llm.prompts import gen_qa_prompt, generate_question_evaluation_prompt, generate_answer_evaluation_prompt\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_endpoint = OpenAIEndpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\\n  \"status\": \"awake\",\\n  \"message\": \"Yes, I\\'m here to assist you!\"\\n}', CompletionUsage(completion_tokens=23, prompt_tokens=24, total_tokens=47, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "resp = openai_endpoint.query(\"Are you up? reply in json\", model_type=OpenAIModelType.OPENAI, model_name=\"gpt4-turbo-0125\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "logger.info(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"preetam7/dynamic_kgqa\")\n",
    "\n",
    "# Work with test split\n",
    "test_data = list(dataset['test'])\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with the QA Generation and Evaluation Pipeline\n",
    "\n",
    "This notebook serves as a **starter template** for our project. It demonstrates the core components of the pipeline and helps you get familiar with how everything fits together.\n",
    "\n",
    "### Overview\n",
    "\n",
    "- **Subgraph Processing**  \n",
    "  The pipeline begins by taking a subgraph as input.\n",
    "\n",
    "- **QA Pair Generation**  \n",
    "  It uses placeholder models (currently GPT-4o via API) to generate question-answer pairs along with supporting paths.\n",
    "\n",
    "- **Evaluation**  \n",
    "  These QA pairs are then evaluated using annotator models. Currently, we use:\n",
    "  - Phi-3\n",
    "  - Two versions of GPT (API-based)\n",
    "\n",
    "### Model Flexibility\n",
    "\n",
    "> **Note:** All models used here are placeholders.  \n",
    "> We plan to replace them with relevant models from [Hugging Face](https://huggingface.co/) to support easier integration and reproducibility.\n",
    "\n",
    "You are encouraged to start with the models mentioned in the paper, but the setup is flexible and can accommodate others as needed.\n",
    "\n",
    "### Infrastructure Support\n",
    "\n",
    "If you need more compute power, I can spin up a **larger VM with better GPUs** â€” just let me know.\n",
    "\n",
    "### What You Should Do\n",
    "\n",
    "- Explore the notebook and run through the workflow\n",
    "- Get familiar with how each component fits into the overall pipeline\n",
    "- Feel free to modify and test with different models\n",
    "- Reach out with questions or suggestions!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'qa_pairs'\n"
     ]
    }
   ],
   "source": [
    "prompt = gen_qa_prompt(test_data[0]['subgraph'])\n",
    "\n",
    "# print(prompt)\n",
    "try:\n",
    "    res, _ = openai_endpoint.query(\"Are you up? reply in json\", model_type=OpenAIModelType.OPENAI, model_name=\"gpt4-turbo-0125\")\n",
    "    res = json.loads(res)\n",
    "    qa_pairs = res['qa_pairs']\n",
    "    for i in range(1): #len(qa_pairs)\n",
    "        question = qa_pairs[i]['question']\n",
    "        answer = qa_pairs[i]['answer']\n",
    "        supporting_facts = qa_pairs[i]['supporting_path']\n",
    "        \n",
    "        question_eval_prompt = generate_question_evaluation_prompt(question)\n",
    "        question_eval_res = get_model_annotations(question_eval_prompt)\n",
    "        print(question_eval_res)\n",
    "        \n",
    "        # Generate and get answer evaluation responses\n",
    "        answer_eval_prompt = generate_answer_evaluation_prompt(question, answer, supporting_facts)\n",
    "        answer_eval_res = get_model_annotations(answer_eval_prompt)\n",
    "        print(answer_eval_res)\n",
    "        \n",
    "        # Get JSON responses directly\n",
    "        judge1_response = answer_eval_res[0]\n",
    "        judge2_response = answer_eval_res[1] \n",
    "        judge3_response = answer_eval_res[2]\n",
    "        \n",
    "        # Get question evaluation responses directly\n",
    "        question_judge1 = question_eval_res[0]\n",
    "        question_judge2 = question_eval_res[1]\n",
    "        question_judge3 = question_eval_res[2]\n",
    "        \n",
    "        # Create a row with same fields as test_data\n",
    "        answer_raw, answer_readable = parse_yago_uri(answer)\n",
    "        row = {\n",
    "            'id': i,\n",
    "            'question': question,\n",
    "            'answer': answer_raw,\n",
    "            'answer_readable': answer_readable,\n",
    "            'answer_uri': answer,  # Original URI\n",
    "            'supporting_facts': json.dumps(supporting_facts),\n",
    "            'supporting_facts_uri': '',  # Would need to extract from supporting facts if available\n",
    "            # Question evaluation fields\n",
    "            'logical_structure_flag_llm_judge1': question_judge1['logical_structure_flag'],\n",
    "            'logical_structure_reasoning_llm_judge1': question_judge1['logical_structure_reasoning'],\n",
    "            'redundancy_flag_llm_judge1': question_judge1['redundancy_flag'],\n",
    "            'redundancy_reasoning_llm_judge1': question_judge1['redundancy_reasoning'],\n",
    "            \n",
    "            'logical_structure_flag_llm_judge2': question_judge2['logical_structure_flag'],\n",
    "            'logical_structure_reasoning_llm_judge2': question_judge2['logical_structure_reasoning'],\n",
    "            'redundancy_flag_llm_judge2': question_judge2['redundancy_flag'],\n",
    "            'redundancy_reasoning_llm_judge2': question_judge2['redundancy_reasoning'],\n",
    "            \n",
    "            'logical_structure_flag_llm_judge3': question_judge3['logical_structure_flag'],\n",
    "            'logical_structure_reasoning_llm_judge3': question_judge3['logical_structure_reasoning'],\n",
    "            'redundancy_flag_llm_judge3': question_judge3['redundancy_flag'],\n",
    "            'redundancy_reasoning_llm_judge3': question_judge3['redundancy_reasoning'],\n",
    "            \n",
    "            # Answer evaluation fields\n",
    "            'answer_support_flag_llm_judge1': judge1_response['answer_support_flag'],\n",
    "            'answer_support_reasoning_llm_judge1': judge1_response['answer_support_reasoning'],\n",
    "            'answer_adequacy_flag_llm_judge1': judge1_response['answer_adequacy_flag'],\n",
    "            'answer_adequacy_reasoning_llm_judge1': judge1_response['answer_adequacy_reasoning'],\n",
    "            \n",
    "            'answer_support_flag_llm_judge2': judge2_response['answer_support_flag'],\n",
    "            'answer_support_reasoning_llm_judge2': judge2_response['answer_support_reasoning'], \n",
    "            'answer_adequacy_flag_llm_judge2': judge2_response['answer_adequacy_flag'],\n",
    "            'answer_adequacy_reasoning_llm_judge2': judge2_response['answer_adequacy_reasoning'],\n",
    "            \n",
    "            'answer_support_flag_llm_judge3': judge3_response['answer_support_flag'],\n",
    "            'answer_support_reasoning_llm_judge3': judge3_response['answer_support_reasoning'],\n",
    "            'answer_adequacy_flag_llm_judge3': judge3_response['answer_adequacy_flag'],\n",
    "            'answer_adequacy_reasoning_llm_judge3': judge3_response['answer_adequacy_reasoning']\n",
    "        }\n",
    "        results.append(row)\n",
    "    # print(res)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which taxon does the alpaca ultimately belong to?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res = json.loads(res[0])\n",
    "\n",
    "res['qa_pairs'][0]['question']\n",
    "# generate_question_evaluation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'question': 'What family does the alpaca belong to, considering its evolutionary lineage through its parent taxa?',\n",
       " 'answer': 'Camelidae',\n",
       " 'answer_readable': 'Camelidae',\n",
       " 'answer_uri': 'http://yago-knowledge.org/resource/Camelidae',\n",
       " 'supporting_facts': '[{\"subject\": \"Alpaca\", \"predicate\": \"parentTaxon\", \"object\": \"Vicugna\"}, {\"subject\": \"Vicugna\", \"predicate\": \"parentTaxon\", \"object\": \"Camelidae\"}]',\n",
       " 'supporting_facts_uri': '[[\"http://yago-knowledge.org/resource/Alpaca\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Vicugna\"], [\"http://yago-knowledge.org/resource/Vicugna\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Camelidae\"]]',\n",
       " 'subgraph': '[[\"http://yago-knowledge.org/resource/Camelidae\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Even-toed_ungulate\"], [\"http://yago-knowledge.org/resource/Even-toed_ungulate\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Ungulate\"], [\"http://yago-knowledge.org/resource/Tragulina\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Ruminant\"], [\"http://yago-knowledge.org/resource/Chevrotain\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Even-toed_ungulate\"], [\"http://yago-knowledge.org/resource/Chevrotain\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Tragulina\"], [\"http://yago-knowledge.org/resource/Camel\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Camelidae\"], [\"http://yago-knowledge.org/resource/Guanaco\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Lama__u0028_genus_u0029_\"], [\"http://yago-knowledge.org/resource/Tylopoda\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Even-toed_ungulate\"], [\"http://yago-knowledge.org/resource/Vicugna\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Camelidae\"], [\"http://yago-knowledge.org/resource/Vicu\\\\u00f1a\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Vicugna\"], [\"http://yago-knowledge.org/resource/Suina\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Even-toed_ungulate\"], [\"http://yago-knowledge.org/resource/Lama__u0028_genus_u0029_\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Camelidae\"], [\"http://yago-knowledge.org/resource/Alpaca\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Vicugna\"], [\"http://yago-knowledge.org/resource/Llama\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Lama__u0028_genus_u0029_\"], [\"http://yago-knowledge.org/resource/Pecora\", \"http://schema.org/parentTaxon\", \"http://yago-knowledge.org/resource/Ruminant\"]]',\n",
       " 'subgraph_size': 15,\n",
       " 'logical_structure_flag_llm_judge1': True,\n",
       " 'logical_structure_reasoning_llm_judge1': 'The question is grammatically correct.',\n",
       " 'redundancy_flag_llm_judge1': False,\n",
       " 'redundancy_reasoning_llm_judge1': 'The question does not contain any obvious redundancies.',\n",
       " 'logical_structure_flag_llm_judge2': True,\n",
       " 'logical_structure_reasoning_llm_judge2': 'The question is grammatically and syntactically correct.',\n",
       " 'redundancy_flag_llm_judge2': False,\n",
       " 'redundancy_reasoning_llm_judge2': 'The question does not contain its own answer explicitly or through overly obvious phrasing.',\n",
       " 'logical_structure_flag_llm_judge3': True,\n",
       " 'logical_structure_reasoning_llm_judge3': 'The grammar and syntax of the question are correct.',\n",
       " 'redundancy_flag_llm_judge3': False,\n",
       " 'redundancy_reasoning_llm_judge3': 'The question does not contain its own answer explicitly or through overly obvious phrasing.',\n",
       " 'answer_support_flag_llm_judge1': True,\n",
       " 'answer_support_reasoning_llm_judge1': \"The answer is explicitly supported by the provided facts. The evolutionary lineage is traced through the parentTaxon predicate, showing that Alpaca's parent taxon is Vicugna, and Vicugna's parent taxon is Camelidae.\",\n",
       " 'answer_adequacy_flag_llm_judge1': True,\n",
       " 'answer_adequacy_reasoning_llm_judge1': 'The answer is adequate and directly responds to the question. The question asks about the family belonging of the alpaca, and the answer Camelidae precisely identifies the family it belongs to.',\n",
       " 'answer_support_flag_llm_judge2': True,\n",
       " 'answer_support_reasoning_llm_judge2': \"The supporting facts indicate that the parent taxon of Alpaca is Vicugna, and the parent taxon of Vicugna is Camelidae. Therefore, the answer 'Camelidae' is supported by the provided facts as it represents the family of the alpaca in its evolutionary lineage.\",\n",
       " 'answer_adequacy_flag_llm_judge2': True,\n",
       " 'answer_adequacy_reasoning_llm_judge2': \"The answer 'Camelidae' directly and fully addresses the question by providing the family to which the alpaca belongs in its evolutionary lineage.\",\n",
       " 'answer_support_flag_llm_judge3': True,\n",
       " 'answer_support_reasoning_llm_judge3': \"The supporting facts indicate that the Alpaca's parent taxon is Vicugna, and Vicugna's parent taxon is Camelidae. This establishes that the Alpaca belongs to the Camelidae family through its evolutionary lineage.\",\n",
       " 'answer_adequacy_flag_llm_judge3': True,\n",
       " 'answer_adequacy_reasoning_llm_judge3': \"The answer 'Camelidae' directly and comprehensively addresses the question about the family to which the alpaca belongs, considering its evolutionary lineage through its parent taxa.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamickgqa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
